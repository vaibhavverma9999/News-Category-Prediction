{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we compare two classifiers Multinomial NB and Logistic Regression\n",
    "#first we import all the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n1 = LabelEncoder()\n",
    "df = pd.read_csv('bbc-text.csv')\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we take text column in X which we will use to predict the category\n",
    "#we take category column in Y which we will use to train our model as output using X\n",
    "X = df['text']\n",
    "X = np.array(X)\n",
    "Y = df['category']\n",
    "Y = np.array(Y)\n",
    "#we vectorize text column using tfidf vectorizer.\n",
    "#tfidf vectorizer works in two parts-tf and idf.\n",
    "#tf is equal to number of occurences of the word in the document divided by total number of words present in that document\n",
    "#idf is equal to log value of number of documents in which that word is present divided by total number of documents\n",
    "#to read in detail about tfidf, kindly visit sklearn documentation pages\n",
    "tfidf = TfidfVectorizer(lowercase=True, analyzer='word', stop_words='english', ngram_range=(1,3), use_idf=True)\n",
    "X1 = tfidf.fit_transform(X)\n",
    "n1 = LabelEncoder()\n",
    "#we vectorize category column using labelencoder\n",
    "#this will give a unique numerical value to every category\n",
    "Y1 = n1.fit_transform(Y)\n",
    "#we separate train and test datasets.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X1, Y1, test_size=0.3, random_state=0)\n",
    "#print(X_train)\n",
    "#print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Multinomial NB is: 93.862275\n"
     ]
    }
   ],
   "source": [
    "#implementing multinomial NB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, Y_train)\n",
    "#print(mnb)\n",
    "test = mnb.predict(X_test)\n",
    "acc = 0\n",
    "l = np.size(test, 0)\n",
    "for i in range(l):\n",
    "    acc = acc + abs(test[i]-Y_test[i])\n",
    "print(\"Accuracy using Multinomial NB is: %f\"%((1-acc/l)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Logistic Regression is: 94.011976\n"
     ]
    }
   ],
   "source": [
    "#implementing Logistic Regression\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "lr.fit(X_train, Y_train)\n",
    "#print(mnb)\n",
    "test = lr.predict(X_test)\n",
    "acc = 0\n",
    "l = np.size(test, 0)\n",
    "for i in range(l):\n",
    "    acc = acc + abs(test[i]-Y_test[i])\n",
    "print(\"Accuracy using Logistic Regression is: %f\"%((1-acc/l)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will implement pickle\n",
    "#we are using pickle to save our models\n",
    "import pickle\n",
    "\n",
    "save_classifier = open(\"naivebayes.pickle\",\"wb\")\n",
    "pickle.dump(mnb, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "save_classifier = open(\"logistic.pickle\",\"wb\")\n",
    "pickle.dump(lr, save_classifier)\n",
    "save_classifier.close()\n",
    "#here we saved both our models in naivebayes.pickle and logistic.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we load naivebayes.pickle and logistic.pickle and use them\n",
    "classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "clf1 = pickle.load(classifier_f)\n",
    "classifier_f.close()\n",
    "\n",
    "classifier_f = open(\"logistic.pickle\", \"rb\")\n",
    "clf2 = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['politics']\n",
      "['politics']\n"
     ]
    }
   ],
   "source": [
    "t = \"Congress leader Nana Patole has been elected as the Speaker of Maharashtra Legislative Assembly after Bharatiya Janata Party (BJP) candidate Kisan Kathore withdrew his nomination on Sunday.\"\n",
    "t = np.array(t).reshape(-1,1)\n",
    "t = tfidf.transform(t[0])\n",
    "#print(t)\n",
    "print(n1.inverse_transform(clf1.predict(t)))\n",
    "print(n1.inverse_transform(clf2.predict(t)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
